\documentclass[a4paper,oneside]{article}

\usepackage{amsmath,amssymb} % amssymb has \lesssim, etc.
\usepackage{graphicx,pict2e,caption,multirow,cleveref}
\usepackage{natbib}
\usepackage{pstricks,pst-node,url}
%\usepackage[breaklinks]{hyperref}

\renewcommand{\familydefault}{\sfdefault}
\usepackage{sfmath}

\usepackage[draft,authormarkup=none,authormarkuptext=none]{changes}
\definechangesauthor[color=red]{m}
\setauthormarkup{} % Gets rid of superposed author id
    % Changes has #1 = author id, #2 = remark, so footnotes only contain remarks, no author id's.
%\setremarkmarkup{\footnote{#2}}

\usepackage[left=2.54cm,right=2.54cm,top=3cm,bottom=3cm]{geometry}

\def\CPPo{{C\kern-.05em\raise.23ex\hbox{+\kern-.05em+}\hspace{0pt}}}
\def\CPP{{C\kern-.05em\raise.23ex\hbox{+\kern-.05em+}\hspace{4pt}}}

\title{Analysis of clusters}
\author{}
\date{}

\begin{document}
\maketitle

The cluster analyses use the output of the {\tt bike-correlations} routines, and can be applied to the five cities of New York, Boston, Chicago,
Washington DC, and London.  The necessary file ``{\tt clusters.cfg}'' has the directory name of this routine. The \CPP routines either read in
({\tt ClustersActual}) or randomly generate ({\tt ClustersNeutral}) cluster memberships, and then simply calculate distances ridden both within
and between all clusters. Both of them calculate results for up to {\tt \_MAX\_CLUST\_SIZE = 100} clusters, with the neutral version averaging
results over {\tt \_NUM\_REPEATS = 100} trials for each number of clusters, producing the files {\tt <city>-results-neutral.txt}.

The {\tt R} routines are packaged as {\tt sortpart}.

\section{Generating Cluster Data}

The \CPP routines are used to generation cluster memberships and to calculate the inter- and intra-cluster distances ridden, according to the
following steps:

\begin{enumerate}
    \item Run\\ {\tt >./ClustersNeutral}\\ for each city to generate neutrally expected values for inter- and intra-cluster distances.
    \item Run the {\tt R} routine\\ 
        {\tt > get.clusters (city, method)}\\
        which generates cluster membership files,\\ {\tt <city>-clust-<to/from>-members-<method>.txt},\\ along with average cluster diameters,\\
        {\tt <city>-clust-<to/from>-diameters-<method>.txt}.\\
        Applicable clustering methods are {\tt complete}, {\tt ward}, and {\tt k-means}, with results from the {\tt skater} method generated
        with\\
        {\tt > get.skater.groups (city)}
    \item Run\\
        {\tt ./ClustersActual}\\ for each city and cluster method. This routine reads the corresponding membership files and produces files
        containing the actual inter- and intra-cluster distances, and named\\ {\tt <city>-results-actual-<to/from>-<method>.txt}.
\end{enumerate}

\section{Preliminary Analyses of Data}

The files generated by {\tt ClustersNeutral} and {\tt ClustersActual} are then preprocessed with the {\tt R} routine\\
{\tt > calc.pnc (city, method, nrpts)}\\
which calculates the probabilities of randomly generating peaks in T-values versus numbers of clusters of the observed heights. The results are
dumped to\\
{\tt <city>-results-prob-m-<method>.txt}.\\
To generate decent statistics, {\tt nrpts} needs to be quite large (around 100,00). This takes quite some time, mostly because of the quantile
regressions and non-linear re-scaling applied to the simulated series.  Pre-processing this time consuming stage nevertheless means that all
remaining analyses can be run reasonably quickly.

\section{The {\tt R} Package: {\tt sortpart}}

Along with the above-mentioned {\tt calc.pnc}, {\tt sortpart} contains the following routines (in alphbetical order):\\
\begin{enumerate}
    \item {\tt calc.pnc : function (city = `nyc', method = `complete',\\ nrpts = 100, rescale = 2)}\\ Calculates probabilities of peak heights as
            returned from the num.clusts () function.  If is.na (dat), then the table is called internally. To generate useful statistics, this
            function should be run with a VERY large number of repeats (for example, 100,000), which will take a LONG time!
    \item {\tt clust.sig : function (city = `nyc', method = `complete', xmax = 36)}\\ Produces the final statistical summary of results for
        a given city and clustering method, which is the ultimate object of the entire analysis.
    \item {\tt get.clusters : function (city = `nyc', method = `ward', trial = ``)}\\
        As mentioned above, this is run as part of the initial data generation phase to generate text files detailing cluster memberships. The
        actual clustering is performed with \ldots
    \item {\tt get.members : function (city = `nyc', nc = 8, method = `ward', details = FALSE)}\\
        Creates clusters and then applies spatial constraint to reallocate stray points to neighbouring clusters.  All methods other than
        k-means are passed to hclust.
    \item {\tt get.num.clusts : function (city = `nyc', method = `complete')}\\
        Just loads up the output of calc.pnc(, and returns the final numbers of clusters corresponding to minimal joint probabilities of peak
        spacings and depths.  
    \item {\tt get.partition.neighbours : function (city = `nyc', method = `complete',\\
        nc = c(11, 15), dir = `from', plot = FALSE)}\\ 
        Identifies the clusters in min (nc) that split to form separate clusters in max (nc). For each cluster, a list element is returned with
        the number of groups into which each connected group is subsequently partitioned.
    \item {\tt get.skater.groups : function (city = `nyc', dirf = `from', max.groups = 50)}\\
        The {\tt skater} version of what {\tt get.clusters} does for the other three clustering methods.
    \item {\tt get.total.dist : function (city = `nyc')}\\
        Simply calculates the total distance ridden for each city.
    \item {\tt num.clusts : function (city = `nyc', plot = FALSE, method = `complete')}\\
        Calculates peak height and G-values as a function of numbers of clusters, to determine the number of clusters that should be used for
        each of the four data series. For {\tt plot=TRUE}, it also reads the table of the same values (from ``{\tt results\_prob\_m.txt}'') to plot
        the probabilities of the observed peak heights, as calcualted from {\tt calc.pnc}. 
        
        These p-values are then used to determine the appropriate number of (most highly significant) peaks to be used for analysing each series
        in the main {\tt clust.sig} routine.
    \item {\tt poly.rescale : function (xm, rescale = 2)}\\
        Applies quadratic regression to input series to rescale peak values between -1 and 1.
\end{enumerate}
\pagebreak


\section{The Entire Procedure}

For any given city and clustering method, the analyses requires the following steps:
\begin{enumerate}
    \item {\tt >./ClustersNeutral city}
    \item {\tt R> get.clusters (city, method)}
    \item {\tt R> get.skater.groups (city, method)}
    \item {\tt >./ClustersActual city}
    \item {\tt R> calc.pnc (city, method, nrpts$\sim$100,000)}
    \item {\tt R> clust.sig (city, method)}
    \item {\tt >./rhier}
\end{enumerate}
The last of these ({\tt rhier}) is a \CPP routine that calculates probabilities of randomly generating clusters with the largest contiguous
component equal to or larger than the observed sizes of contiguous components. These latter sizes must be entered manually in the code, which
must be re-compiled for each city. (There are generally only a handful of entries, so this is a simple work-around.)


\end{document}
